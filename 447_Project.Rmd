---
title: "STAT 447C Project: Bora Guney and Damien Fung"
output:
  pdf_document: default
  html_document: default
date: "2024-04-09"
fontsize: 9pt
---

```{css, echo=FALSE}
p {
  font-size: 32px;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(tidyverse)

# Data Preparation
df_three <- read.table("./data-three_years.txt")
df_five <- read.table("./data-five_years.txt")
df_three$V1 = as.Date(df_three$V1)
df_five$V1 = as.Date(df_five$V1)

#df_three <- df_three %>%
#  group_by(year = year(df_three$V1), month = month(df_three$V1)) %>%
#  summarize(monthly_index = mean(V2))

#df_five <- df_five %>%
#  group_by(year = year(df_five$V1), month = month(df_five$V1)) %>%
#  summarize(monthly_index = mean(V2))

x.ts.3 = ts(df_three$V2, start = c(2021, 4), freq = 251)
x.ts.5 = ts(df_five$V2, start = c(2019, 4), freq = 251)

x.ts.3.train = window(x.ts.3, start=c(2021,4), end=c(2024,3), frequency = 251)
x.ts.5.train = window(x.ts.5, start=c(2019,4), end=c(2024,3), frequency = 251)

# we use the same one for both time series since the data is identical
x.ts.test = window(x.ts.3, start=c(2024,4), end=c(2024,5), frequency = 251)
```

# Introduction

Throughout the duration of this outgoing academic term, we have both been undergoing STAT 443: Time Series & Forecasting. Taking both STAT 443 and STAT 447C in tandem, we have grown a keen interesting in the utilization of Bayesian techniques within time series forecasting. Namely, this project aims to compare parameter estimates and prediction performance obtained using Bayesian techniques compared to those obtained using SARIMA. To demonstrate this, we utilize the S&P 500's historic data from the previous 3 years. We develop our Bayesian model using MH MCMC to learn model parameters for the 3 year data set. Furthermore, we employ a hierarchical model where we treat the 5 year data set as side data to inform our prior choices. By completing this project, we aim to determine the better approach to predicting economic patterns despite their chaotic nature.

# Preliminary EDA and Data Preparation

To determine model performance without data leakage, we reserve the most recent month (Mar 2024 to Apr 2024) of S&P 500 data as an unseen test set. As such, we conduct our EDA and data preparation with the remaining data.

```{r, echo = FALSE, out.width="50%"}

#print("S&P 500 Index data over 3 years")
#head(df_three)
#print("S&P 500 Index data over 5 years")
#head(df_five)

plot(x.ts.3.train, ylab = "S&P 500 Index", main = "S&P 500 Index over 3 years")
plot(x.ts.5.train, ylab = "S&P 500 Index", main = "S&P 500 Index over 5 years")
```

## Detrending and deseasonalizing the data

The time series plots shown for both 3 year and 5 year data sets demonstrate that the data is not stationary (mean is not constant); namely some trend and seasonality may be observed. As a result of this, we remove trend and seasonal variation from both datasets. 


```{r, echo = FALSE, out.width="33%", out.height="150%"}
x.ts.3.stl <- stl(x.ts.3.train, s.window = "periodic")
x.ts.3.dst <- x.ts.3.train - x.ts.3.stl$time.series[,"seasonal"] - x.ts.3.stl$time.series[,"trend"]

x.ts.5.stl <- stl(x.ts.5.train, s.window = "periodic")
x.ts.5.dst <- x.ts.5.train - x.ts.5.stl$time.series[,"seasonal"] - x.ts.3.stl$time.series[,"trend"]

plot(x.ts.3.dst, main = "Deseasonalized & detrended S&P 500 Index over 3 years")
acf(x.ts.3.dst, lag.max = 500, main = "ACF of deseasonalized/detrended S&P 500 Index over 3 years")
pacf(x.ts.3.dst, lag.max = 500, main = "PACF of deseasonalized/detrended S&P 500 Index over 3 years")

plot(x.ts.5.dst, main = "Deseasonalized & detrended S&P 500 Index over 5 years")
acf(x.ts.5.dst, lag.max = 500, main = "ACF of deseasonalized S&P 500 index over 5 years")
pacf(x.ts.5.dst, lag.max = 500, main = "PACF of deseasonalized S&P 500 index over 5 years")
```

## Applying differencing for SARIMA

Despite the deseasonalization and detrending of the data, the time series still exhibits some seasonality and trend. Therefore, we perform differencing at lag one and seasonal differencing at lag 251 (since we seem to have a seasonal period of 1 year. Note that the S&P 500 does not include data on days where the market is closed) in an attempt to obtain a stationary time series.

```{r, echo = FALSE, out.width="33%", out.height="150%"}
x.ts.3.diff1 = diff(x.ts.3.dst, lag = 1)
x.ts.3.diff2 = diff(x.ts.3.diff1, lag = 251)

plot(x.ts.3.diff2, main = "S&P 500 Index over 3 years after seasonal differencing", ylab = "S&P 500 Index (after seasonal differencing)")
acf(x.ts.3.diff2, lag.max = 500, main = "ACF of S&P 500 Index over 3 years after seasonal differencing")
pacf(x.ts.3.diff2, lag.max = 500, main = "PACF of S&P 500 Index over 3 years after seasonal differencing")

x.ts.5.diff1 = diff(x.ts.5.dst, lag = 1)
x.ts.5.diff2 = diff(x.ts.5.diff1, lag = 251)

plot(x.ts.5.diff2, main = "S&P 500 Index over 5 years after seasonal differencing", ylab = "S&P 500 Index (after seasonal differencing)")
acf(x.ts.5.diff2, lag.max = 720, main = "ACF of S&P 500 Index over 5 years after seasonal differencing")
pacf(x.ts.5.diff2, lag.max = 720, main = "PACF of S&P 500 Index over 5 years after seasonal differencing")
```

After the application of differencing on both time series, they may be considered stationary.


=======
## SARIMA Model

Using the EDA and prepared data from the previous section, we can determine the appropriate $SARIMA(p,d,q)(P,D,Q)_s$ model accordingly. From the application of differencing, we have determined that $d = 1, D = 1, s = 251$. By the sample ACFs and PACFs, lag values before the 1.0 lag marker are above the significance line while lag values after are consistently under. This implies that $P = 0$ and $Q = 1$. To determine the ideal p and q values given that the remaining parameters in the $SARIMA$ model are set as described, we find the values of $p, q$ that yield the lowest AIC

```{r, echo = FALSE}
out.aic <-matrix(nrow=4, ncol=4)
for(p in 0:3) {for(q in 0:3){
fm <- arima(x.ts.5.train, order=c(p,1,q), seasonal=list(order=c(0,1,1)),include.mean = FALSE)
out.aic[p+1,q+1] <-AIC(fm)
}
}
order_ind =which(out.aic== min(out.aic), arr.ind = TRUE)
p <- order_ind[1]-1
q <- order_ind[2]-1
```


# Model Development
We start by constructing a Bayesian model for the three year data, then we will
make the model more complex. Acf and pacf analysis of the data showed that an 
$AR(1)$ process is suitable. Therefore, we incorporate this knowledge into our 
Bayesian model. Let $i$ be the time index and $y$ be the deaseasonalized time 
series. 


$$\sigma \sim Exp\left(\frac{1}{100} \right)$$

$$\alpha \sim N(0,10)$$
$$y_i \sim N \left(\alpha \cdot y_{i-1}, \sigma \right)$$


```{stan output.var= "sp_model"}
data {
int N; // number of samples
real initial;
real final;
vector [N] y; // SP 500 index value
}
parameters {
  real <lower = 0> sigma;
  real alpha;
}

model {
  sigma ~ exponential(1.0/100);
  alpha ~ normal(0, 10);
  for (i in 1:N){
    if (i == 1){
    y[i] ~ normal(alpha*initial, sigma);
} else{
  y[i] ~ normal(alpha*y[i-1], sigma);
}

}

}

generated quantities {
  vector [2] yhat;
  for (i in 1:2){
  if (i == 1){
  yhat[i] = normal_rng(alpha*final, sigma);
} else{
  yhat[i] =  normal_rng(alpha*yhat[i-1], sigma);
  
}
  
}

}
```


We use the data from April $2021$ to April $4$th $2024$ for training. Further, 
the data from April $5$th and $8$th$2024$ will be used for testing.

```{r, echo = FALSE, out.width="33%", out.height="150%"}
suppressMessages(require(rstan))
train = x.ts.3.train

fit = sampling(
  sp_model,
  seed = 1,
  chains = 1,
  data = list(
    N = length(train),
    initial = train[1],
    final = train[753],
    y = train
  ),
  iter = 10000
)

```

```{r, echo = FALSE, out.width="33%", out.height="150%"}
samples = extract(fit)
fit
yhats = colMeans(samples$yhat)
```

We plot the predictions, actual values, and $95 \%$ confidence intervals as 
follows :
```{r, echo = FALSE, out.width="33%", out.height="150%"}
cı.yhat1 = quantile(samples$yhat[,1], c(0.025, 0.975))
cı.yhat2 = quantile(samples$yhat[,2], c(0.025, 0.975))
preds.ts = ts(yhats, start = as.Date("2024-4-5"), end = as.Date("2024-4-8"))
dates = c(as.Date("2024-4-5"), as.Date("2024-4-8"))
plot(x = dates, y = c(preds.ts[1], preds.ts[2]), main = "Predictions vs Time", 
     type = "o", ylim = c(5010, 5280), col = "purple", ylab = "Index Value")
lines(x = dates, y = c(x.ts.test[1], x.ts.test[2]), type = "o")
lines(x = dates, y = c(cı.yhat1[1], cı.yhat2[1]), type = "o", col = "purple", lty = 2)
lines(x = dates, y = c(cı.yhat1[2], cı.yhat2[2]), type = "o", col = "purple", lty = 2)
legend("topleft", legend = c("Predictions","Actual Values", "Predictions Intervals"), 
       lty = c(1,1,2), col = c("purple", "black", "purple"))
```



Now, we will use the five year data, and will try to find 
the change points within the data. From the time series plot of the five year data, 
it seems that there are multiple change point candidates, one of them being around 
COVID times. We assume that there are two change points, and therefore three 
$\alpha$ values corresponding the divided regions. Furher, we assume that 
each region has the same standard deviation for simplicity. 
To find the change point we use the following alternation of Kernels model :

$$K = K_1 \circ K_2 \circ K_3$$
, where $K_1$ only modifies $\sigma$, $K_2$ only modifies the $\alphas$'s, and
$K_3$ only modifies the change points. We know that if $K_1$, $K_2$, and $K_3$
are $\pi$-invariant, then $K$ is too. For this particular target distribution, we can 
say that $K_1$ is irreducible if the proposal for $K_1$ can reach all of $R_{+}$
from any starting point. Further, $K_2$ is irreducible if the the proposal for 
$K_2$ can reach all of $R_{+}^3$ from any starting point. Lastly, $K_3$ is irreducible
if the proposal for $K_3$ can reach all of $\{1,2,..., 627\} \times \{628, 629,..., 1255\}$ 
from any starting point. For the sigma, we use a normal distribution centered at
the current $\sigma$:
$$\sigma \sim N()$$
```{r, echo = FALSE, out.width="33%", out.height="150%"}
set.seed(1)

log_joint = function(sigma, alphas, change_point, y) {
  
  # Return log(0.0) if parameters are outside of the support
  if (sigma < 0) 
    return(-Inf)
  
  log_prior = 
    dexp(sigma, 1/100, log = TRUE) +
    dnorm(alphas[[1]], 1, 0.001, log = TRUE) +
    dnorm(alphas[[2]], 1, 0.001, log = TRUE) +
    dnorm(alphas[[3]], 1, 0.001, log = TRUE) +
    dnorm(alphas[[4]], 1, 0.001, log = TRUE) +
    dnorm(alphas[[5]], 1, 0.001, log = TRUE)
  
  log_likelihood = 0.0
  for (i in 1:length(y)) {
     if (i < change_point[1]){
      alpha = alphas[[1]]
      
    }  else if(change_point[1] <= i & i < change_point[2]){
      alpha = alphas[[2]]
    } else if (change_point[2] <= i & i < change_point[3]){
      alpha = alphas[[3]]
    } else if (change_point[3] <= i & i < change_point[4]){
      alpha = alphas[[4]]
    }
    else {
      alpha = alphas[5]
    }
    if (i == 1){
      log_likelihood = log_likelihood + dnorm(y[[i]], alpha*y[[i]], sigma, log = TRUE)
    } else{
      log_likelihood = log_likelihood + dnorm(y[[i]], alpha*y[[i-1]], sigma, log = TRUE)
      
    }
  }
  
  return(log_prior + log_likelihood)
}
```

Then we specify three kernels, one for $sigma$,  $\alpha$, and change point.

```{r}
kernel1 = function(gamma, current_point, alphas, proposal_sd, change.p, y) {
dim = length(current_point)
proposal = rnorm(dim, mean = current_point, sd = proposal_sd)
ratio = exp(gamma(proposal, alphas, change.p,y ) - gamma(current_point, alphas, change.p, y))
if (runif(1) < ratio & ratio != Inf) {
return(proposal)
} else {
return(current_point)
}
}
```



```{r}
kernel2 = function(gamma, sigma, current_point, proposal_sd, change.p, y) {
dim = length(current_point)
proposal = rnorm(dim, mean = current_point, sd = proposal_sd)
ratio = exp(gamma(sigma, proposal, change.p,y) - gamma(sigma, current_point, change.p, y))
if (runif(1) < ratio & ratio != Inf) {
return(proposal)
} else {
return(current_point)
}
}
```



```{r}
kernel3 = function(gamma, sigma, alphas, current_point, y) {
dim = length(current_point)
proposal1 = sample(1:floor(length(y)/4) , size = 1)
proposal2 = sample(seq(floor(length(x.ts.5.train)/4) + 1, 2*floor(length(x.ts.5.train)/4)), size = 1)
proposal3 = sample(seq(2*floor(length(x.ts.5.train)/4) + 1, 3*floor(length(x.ts.5.train)/4)), size = 1)
proposal4 = sample(seq(floor(3*length(x.ts.5.train)/4) + 1, 4*floor(length(x.ts.5.train)/4)), size = 1)
proposal = c(proposal1, proposal2, proposal3, proposal4)
ratio = exp(gamma(sigma, alphas, proposal, y) - gamma(sigma, alphas, current_point, y))
if (runif(1) < ratio & ratio != Inf) {
return(proposal)
} else {
return(current_point)
}
}
```




```{r}
rho_helper = function(){
  rho = runif(1)
  if(rho < 1/3){
    return(1)
  } else if(1/3 <= rho & rho < 2/3){
    return(2)
  } else{
    return(3)
  }
    
    }
mcmc = function(gamma, sigma, alphas, change_point, y, n_iterations){
  change_point_trace1 = rep(-1, n_iterations)
  change_point_trace2 = rep(-1, n_iterations)
  change_point_trace3 = rep(-1, n_iterations)
  change_point_trace4 = rep(-1, n_iterations)
  sigma_trace = rep(-1, n_iterations)
  alpha1_trace = rep(-1, n_iterations)
  alpha2_trace = rep(-1, n_iterations)
  alpha3_trace = rep(-1, n_iterations)
  alpha4_trace = rep(-1, n_iterations)
  alpha5_trace = rep(-1, n_iterations)
  
  for (i in 1:n_iterations) {
    rho = rho_helper() # choose kernel
    
    if (rho == 1){
      sigma = kernel1(log_joint, sigma, alphas, 5, change_point, y)
      change_point_trace1[i] = change_point[1]
      change_point_trace2[i] = change_point[2]
      change_point_trace3[i] = change_point[3]
      change_point_trace4[i] = change_point[4]
      sigma_trace[i] = sigma
    }
    else if(rho == 2){
      alphas =  kernel2(log_joint, sigma, alphas, 0.001, change_point, y)
      change_point_trace1[i] = change_point[1]
      change_point_trace2[i] = change_point[2]
      change_point_trace3[i] = change_point[3]
      change_point_trace4[i] = change_point[4]
      alpha1_trace[i] = alphas[1]
      alpha2_trace[i] = alphas[2]
      alpha3_trace[i] = alphas[3]
      alpha4_trace[i] = alphas[4]
      alpha5_trace[i] = alphas[5]
      
} 
    else{
      change_point = kernel3(log_joint, sigma, alphas, change_point, y)
      change_point_trace1[i] = change_point[1]
      change_point_trace2[i] = change_point[2]
      change_point_trace3[i] = change_point[3]
      change_point_trace4[i] = change_point[4]
      
    }
  }
  
  return(
    list(
      
      change_point_trace1 = change_point_trace1,
      change_point_trace2 = change_point_trace2,
      change_point_trace3 = change_point_trace3,
      change_point_trace4 = change_point_trace4,
      last_iteration_sigma = sigma,
      last_iteration_alphas = alphas,
      sigma_trace = sigma_trace,
      alpha1_trace = alpha1_trace,
      alpha2_trace = alpha2_trace,
      alpha3_trace = alpha3_trace,
      alpha4_trace = alpha4_trace,
      alpha5_trace = alpha5_trace
    )
  )
}
```




```{r}
set.seed(12)
output = suppressWarnings(mcmc(log_joint, 45, c(1,1,1,1,1), c(200, 500, 800, 1100), x.ts.5.train, 20000))
```


```{r}
hist(output$change_point_trace1[output$change_point_trace1 != -1])
hist(output$change_point_trace2[output$change_point_trace2 != -1])
hist(output$change_point_trace3[output$change_point_trace3 != -1])
hist(output$change_point_trace4[output$change_point_trace4 != -1])
```



```{r}
mean(output$sigma_trace[output$sigma_trace != -1])
mean(output$alpha1_trace[output$alpha1_trace != -1])
mean(output$alpha2_trace[output$alpha2_trace != -1])
mean(output$alpha3_trace[output$alpha3_trace != -1])
mean(output$alpha4_trace[output$alpha4_trace != -1])
mean(output$alpha5_trace[output$alpha5_trace != -1])
```



