---
title: "STAT 447C Project: Bora Guney and Damien Fung"
output:
  pdf_document: default
  html_document: default
date: "2024-04-09"
fontsize: 9pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(tidyverse)

# Data Preparation
df_three <- read.table("./data-three_years.txt")
df_five <- read.table("./data-five_years.txt")
df_three$V1 = as.Date(df_three$V1)
df_five$V1 = as.Date(df_five$V1)

df_three <- df_three %>%
  group_by(year = year(df_three$V1), month = month(df_three$V1)) %>%
  summarize(monthly_index = mean(V2))

df_five <- df_five %>%
  group_by(year = year(df_five$V1), month = month(df_five$V1)) %>%
  summarize(monthly_index = mean(V2))

x.ts.3 = ts(df_three$monthly_index, start = c(2021, 4), freq = 12)
x.ts.5 = ts(df_five$monthly_index, start = c(2019, 4), freq = 12)
```

# Introduction

Throughout the duration of this outgoing academic term, we have both been undergoing STAT 443: Time Series & Forecasting. Taking both STAT 443 and STAT 447C in tandem, we have grown a keen interesting in the utilization of Bayesian techniques within time series forecasting. Namely, this project aims to compare parameter estimates and prediction performance obtained using Bayesian techniques compared to those obtained using ARIMA. To demonstrate this, we utilize the S&P 500's historic data from the previous 3 years. We develop our Bayesian model using MH MCMC to learn model parameters for the 3 year data set. Furthermore, we employ a hierarchical model where we treat the 5 year data set as side data to inform our prior choices. By completing this project, we aim to determine the better approach to predicting economic patterns despite their chaotic nature.

# Preliminary EDA and Data Preparation

To simplify the frequentist (ARIMA) model creation, we take the monthly average of both the 3 year and the 5 year data sets instead of the daily value of the S&P 500.

```{r, echo = FALSE, out.width="50%"}

#print("S&P 500 Index data over 3 years")
#head(df_three)
#print("S&P 500 Index data over 5 years")
#head(df_five)

plot(x.ts.3, ylab = "S&P 500 Index", main = "S&P 500 Index over 3 years (Monthly Average)")
plot(x.ts.5, ylab = "S&P 500 Index", main = "S&P 500 Index over 5 years (Monthly Average)")
```

The time series plots shown for both 3 year and 5 year data sets demonstrate that the data is not stationary (mean is not constant), and some seasonality may be observed. As such, we remove seasonal variation from both datasets.

```{r, echo = FALSE, out.width="50%"}
x.ts.3.stl <- stl(x.ts.3, s.window = "periodic")
x.ts.3.ds <- x.ts.3 - x.ts.3.stl$time.series[,"seasonal"]

x.ts.5.stl <- stl(x.ts.5, s.window = "periodic")
x.ts.5.ds <- x.ts.5 - x.ts.5.stl$time.series[,"seasonal"]

plot(x.ts.3.ds, main = "Deseasonalized S&P 500 Index over 3 years")
plot(x.ts.5.ds, main = "Deseasonalized S&P 500 Index over 5 years")
```

The autocorrelation functions (ACF) for both time series seem to have a sinusoidal pattern and tails off at around lag 1 (approximately 1 year) whereas their partial autocorrelation functions (PACF) seem to abruptly cut off after a lag of 1 month. As such, this suggests that the time series for both the 3 year and 5 year dataset could be represented as an AR(1) process

```{r, echo = FALSE, out.width="50%"}
acf(x.ts.3.ds, lag.max = 24, main = "ACF of Deseasonalized S&P 500 Index over 3 years")
pacf(x.ts.3.ds, lag.max = 24, main = "PACF of Deseasonalized S&P 500 Index over 3 years")

acf(x.ts.5.ds, lag.max = 24, main = "ACF of Deseasonalized S&P 500 Index over 5 years")
pacf(x.ts.5.ds, lag.max = 24, main = "PACF of Deseasonalized S&P 500 Index over 5 years")
```

# Model Development

We start by constructing a Bayesian model for the three year data, then we will
make the model more complex. Acf and pacf analysis of the data showed that an 
$AR(1)$ process is suitable. Therefore, we incorporate this knowledge into our 
Bayesian model. Let $i$ be the time index and $y$ be the deaseasonalized time 
series. 


$$\sigma \sim Exp \left(\frac{1}{100} \right)$$
$$y_i \sim N \left(y_{i-1}, \sigma \right)$$


```{stan output.var= "sp_model"}
data {
int N; // number of samples
real initial;
real final;
vector [N] y; // SP 500 index value
}
parameters {
  real <lower = 0> sigma;
}

model {
  sigma ~ exponential(1.0/100);
  for (i in 1:N){
    if (i == 1){
    y[i] ~ normal(initial, sigma);
} else{
  y[i] ~ normal(y[i-1], sigma);
}

}

}

generated quantities {
  vector [4] yhat;
  for (i in 1:4){
  if (i == 1){
  yhat[i] = normal_rng(final, sigma);
} else{
  yhat[i] =  normal_rng(yhat[i-1], sigma);
  
}
  
}

}
```


We use the data from April $2021$ to December $2023$ for training. Further, the data
from January $2023$ to April $2024$ will be used for testing.

```{r}
suppressMessages(require(rstan))
train = x.ts.3.ds[1:33] 

fit = sampling(
  sp_model,
  seed = 1,
  chains = 1,
  data = list(
    N = length(train),
    initial = train[1],
    final = train[33],
    y = train
  ),
  iter = 10000
)

```

```{r}
samples = extract(fit)
fit
yhats = colMeans(samples$yhat)
```

```{r}
preds.ts = ts(yhats, start = c(2024, 01), end = c(2024,04), frequency = 12)
plot(preds.ts)
```

